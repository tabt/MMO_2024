{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPeIGo3uytMsmW03NN81Y/c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install gym==0.26.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":272},"id":"iIg3wXqOuMzk","executionInfo":{"status":"ok","timestamp":1717764174373,"user_tz":-180,"elapsed":1321,"user":{"displayName":"Roman Kuzmin","userId":"11647731679283067401"}},"outputId":"fd5a742f-0e11-4a76-8ddb-6c980bc979e3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing collected packages: gym\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gym-0.26.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gym"]},"id":"d8c56a1d7d6a4fe9aa52fea7e54afc3d"}},"metadata":{}}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tev-nk0kfdjl","executionInfo":{"status":"ok","timestamp":1717765409305,"user_tz":-180,"elapsed":109127,"user":{"displayName":"Roman Kuzmin","userId":"11647731679283067401"}},"outputId":"b619ec86-accb-4406-b5e0-e8f0d49ab25e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Стратегия:\n","array([[0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25],\n","       [0.25, 0.25, 0.25, 0.25]])\n","Алгоритм выполнился за 1000 шагов.\n","Стратегия:\n","array([[0.        , 0.5       , 0.5       , 0.        ],\n","       [0.33333333, 0.33333333, 0.33333333, 0.        ],\n","       [0.        , 0.        , 1.        , 0.        ],\n","       [0.        , 0.        , 1.        , 0.        ],\n","       [0.        , 0.        , 1.        , 0.        ],\n","       [0.        , 0.        , 1.        , 0.        ],\n","       [0.        , 0.        , 1.        , 0.        ],\n","       [0.        , 0.        , 1.        , 0.        ],\n","       [0.        , 0.        , 1.        , 0.        ],\n","       [0.        , 0.        , 1.        , 0.        ],\n","       [0.33333333, 0.        , 0.33333333, 0.33333333],\n","       [0.        , 0.        , 0.5       , 0.5       ],\n","       [0.        , 0.        , 1.        , 0.        ],\n","       [0.        , 0.5       , 0.5       , 0.        ],\n","       [0.        , 0.5       , 0.5       , 0.        ],\n","       [0.        , 0.33333333, 0.33333333, 0.33333333],\n","       [0.        , 0.33333333, 0.33333333, 0.33333333],\n","       [0.        , 0.33333333, 0.33333333, 0.33333333],\n","       [0.        , 0.33333333, 0.33333333, 0.33333333],\n","       [0.        , 0.33333333, 0.33333333, 0.33333333],\n","       [0.        , 0.33333333, 0.33333333, 0.33333333],\n","       [0.        , 0.        , 0.5       , 0.5       ],\n","       [0.        , 0.        , 0.5       , 0.5       ],\n","       [0.        , 0.        , 1.        , 0.        ],\n","       [0.        , 0.33333333, 0.33333333, 0.33333333],\n","       [0.        , 0.5       , 0.        , 0.5       ],\n","       [0.33333333, 0.33333333, 0.        , 0.33333333],\n","       [0.33333333, 0.33333333, 0.        , 0.33333333],\n","       [0.33333333, 0.33333333, 0.        , 0.33333333],\n","       [0.33333333, 0.33333333, 0.        , 0.33333333],\n","       [0.33333333, 0.33333333, 0.        , 0.33333333],\n","       [0.33333333, 0.33333333, 0.        , 0.33333333],\n","       [0.33333333, 0.33333333, 0.        , 0.33333333],\n","       [0.33333333, 0.33333333, 0.        , 0.33333333],\n","       [0.        , 0.5       , 0.        , 0.5       ],\n","       [0.        , 0.33333333, 0.33333333, 0.33333333],\n","       [0.33333333, 0.        , 0.33333333, 0.33333333],\n","       [0.5       , 0.        , 0.        , 0.5       ],\n","       [1.        , 0.        , 0.        , 0.        ],\n","       [1.        , 0.        , 0.        , 0.        ],\n","       [1.        , 0.        , 0.        , 0.        ],\n","       [1.        , 0.        , 0.        , 0.        ],\n","       [1.        , 0.        , 0.        , 0.        ],\n","       [1.        , 0.        , 0.        , 0.        ],\n","       [1.        , 0.        , 0.        , 0.        ],\n","       [1.        , 0.        , 0.        , 0.        ],\n","       [0.5       , 0.5       , 0.        , 0.        ],\n","       [0.33333333, 0.33333333, 0.33333333, 0.        ]])\n"]}],"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pprint import pprint\n","import pandas as pd\n","from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n","\n","\n","def print_full(x):\n","    pd.set_option('display.max_rows', len(x))\n","    print(x)\n","    pd.reset_option('display.max_rows')\n","\n","class PolicyIterationAgent:\n","    '''\n","    Класс, эмулирующий работу агента\n","    '''\n","    def __init__(self, env):\n","        self.env = env\n","        # Пространство состояний\n","        self.observation_dim = 48\n","        # Массив действий в соответствии с документацией\n","        # https://www.gymlibrary.dev/environments/toy_text/cliff_walking/\n","        self.actions_variants = np.array([0,1,2,3])\n","        # Задание стратегии (политики)\n","        self.policy_probs = np.full((self.observation_dim, len(self.actions_variants)), 0.25)\n","        # Начальные значения для v(s)\n","        self.state_values = np.zeros(shape=(self.observation_dim))\n","        # Начальные значения параметров\n","        self.maxNumberOfIterations = 1000\n","        self.theta=1e-6\n","        self.gamma=0.99\n","\n","\n","    def print_policy(self):\n","        '''\n","        Вывод матриц стратегии\n","        '''\n","        print('Стратегия:')\n","        pprint(self.policy_probs)\n","\n","\n","    def policy_evaluation(self):\n","        '''\n","        Оценивание стратегии\n","        '''\n","        # Предыдущее значение функции ценности\n","        valueFunctionVector = self.state_values\n","        for iterations in range(self.maxNumberOfIterations):\n","            # Новое значение функции ценности\n","            valueFunctionVectorNextIteration=np.zeros(shape=(self.observation_dim))\n","            # Цикл по состояниям\n","            for state in range(self.observation_dim):\n","                # Вероятности действий\n","                action_probabilities = self.policy_probs[state]\n","                # Цикл по действиям\n","                outerSum=0\n","                for action, prob in enumerate(action_probabilities):\n","                    innerSum=0\n","                    # Цикл по вероятностям действий\n","                    for probability, next_state, reward, isTerminalState in self.env.P[state][action]:\n","                        innerSum=innerSum+probability*(reward+self.gamma*self.state_values[next_state])\n","                    outerSum=outerSum+self.policy_probs[state][action]*innerSum\n","                valueFunctionVectorNextIteration[state]=outerSum\n","            if(np.max(np.abs(valueFunctionVectorNextIteration-valueFunctionVector))<self.theta):\n","                # Проверка сходимости алгоритма\n","                valueFunctionVector=valueFunctionVectorNextIteration\n","                break\n","            valueFunctionVector=valueFunctionVectorNextIteration\n","        return valueFunctionVector\n","\n","\n","    def policy_improvement(self):\n","        '''\n","        Улучшение стратегии\n","        '''\n","        qvaluesMatrix=np.zeros((self.observation_dim, len(self.actions_variants)))\n","        improvedPolicy=np.zeros((self.observation_dim, len(self.actions_variants)))\n","        # Цикл по состояниям\n","        for state in range(self.observation_dim):\n","            for action in range(len(self.actions_variants)):\n","                for probability, next_state, reward, isTerminalState in self.env.P[state][action]:\n","                    qvaluesMatrix[state,action]=qvaluesMatrix[state,action]+probability*(reward+self.gamma*self.state_values[next_state])\n","\n","            # Находим лучшие индексы\n","            bestActionIndex=np.where(qvaluesMatrix[state,:]==np.max(qvaluesMatrix[state,:]))\n","            # Обновление стратегии\n","            improvedPolicy[state,bestActionIndex]=1/np.size(bestActionIndex)\n","        return improvedPolicy\n","\n","\n","    def policy_iteration(self, cnt):\n","        '''\n","        Основная реализация алгоритма\n","        '''\n","        policy_stable = False\n","        for i in range(1, cnt+1):\n","            self.state_values = self.policy_evaluation()\n","            self.policy_probs = self.policy_improvement()\n","        print(f'Алгоритм выполнился за {i} шагов.')\n","\n","\n","def play_agent(agent):\n","    env2 = gym.make('CliffWalking-v0', render_mode='human')\n","    state = env2.reset()[0]\n","    done = False\n","    while not done:\n","        p = agent.policy_probs[state]\n","        if isinstance(p, np.ndarray):\n","            action = np.random.choice(len(agent.actions_variants), p=p)\n","        else:\n","            action = p\n","        next_state, reward, terminated, truncated, _ = env2.step(action)\n","        env2.render()\n","        state = next_state\n","        if terminated or truncated:\n","            done = True\n","\n","\n","def main():\n","    # Создание среды\n","    env = gym.make('CliffWalking-v0')\n","    env.reset()\n","    # Обучение агента\n","    agent = PolicyIterationAgent(env)\n","    agent.print_policy()\n","    agent.policy_iteration(1000)\n","    agent.print_policy()\n","    # Проигрывание сцены для обученного агента\n","    play_agent(agent)\n","\n","\n","if __name__ == '__main__':\n","    main()"]}]}