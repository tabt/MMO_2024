{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOMqBGhHZaFrZRsc1VT4ofl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["! pip install gym==0.26.2"],"metadata":{"id":"Xjn-6LBH70Xx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"b90y9siwswml","executionInfo":{"status":"ok","timestamp":1717851635679,"user_tz":-180,"elapsed":1719,"user":{"displayName":"Roman Kuzmin","userId":"11647731679283067401"}}},"outputs":[],"source":["import gym\n","import math\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple, deque\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.optim.lr_scheduler as lr_scheduler"]},{"cell_type":"code","source":["# Название среды\n","CONST_ENV_NAME = 'CartPole-v1'\n","# Использование GPU\n","CONST_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Элемент ReplayMemory в форме именованного кортежа\n","Transition = namedtuple('Transition',\n","                        ('state', 'action', 'next_state', 'reward'))"],"metadata":{"id":"0MZnpRiU68HT","executionInfo":{"status":"ok","timestamp":1717851635679,"user_tz":-180,"elapsed":3,"user":{"displayName":"Roman Kuzmin","userId":"11647731679283067401"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Реализация техники Replay Memory\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.memory = deque([], maxlen=capacity)\n","\n","    def push(self, *args):\n","        '''\n","        Сохранение данных в ReplayMemory\n","        '''\n","        self.memory.append(Transition(*args))\n","\n","    def sample(self, batch_size):\n","        '''\n","        Выборка случайных элементов размера batch_size\n","        '''\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","\n","class DQN_Model(nn.Module):\n","    def __init__(self, n_observations, n_actions):\n","        '''\n","        Инициализация топологии нейронной сети\n","        '''\n","        super(DQN_Model, self).__init__()\n","        self.layer1 = nn.Linear(n_observations, 128)\n","        self.layer2 = nn.Linear(128, 128)\n","        self.layer3 = nn.Linear(128, n_actions)\n","\n","    def forward(self, x):\n","        '''\n","        Прямой проход\n","        Вызывается для одного элемента, чтобы определить следующее действие\n","        Или для batch'а во время процедуры оптимизации\n","        '''\n","        x = F.relu(self.layer1(x))\n","        x = F.relu(self.layer2(x))\n","        return self.layer3(x)\n","\n","\n","class DQN_Agent:\n","    def __init__(self, env,\n","                 BATCH_SIZE = 64,\n","                 GAMMA = 0.99,\n","                 EPS_START = 0.9,\n","                 EPS_END = 0.05,\n","                 EPS_DECAY = 1000,\n","                 TAU = 0.005,\n","                 LR = 1e-4\n","                 ):\n","        # Среда\n","        self.env = env\n","        # Размерности Q-модели\n","        self.n_actions = env.action_space.n\n","        state, _ = self.env.reset()\n","        self.n_observations = len(state)\n","        # Коэффициенты\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.GAMMA = GAMMA\n","        self.EPS_START = EPS_START\n","        self.EPS_END = EPS_END\n","        self.EPS_DECAY = EPS_DECAY\n","        self.TAU = TAU\n","        self.LR = LR\n","        # Модели\n","        # Основная модель\n","        self.policy_net = DQN_Model(self.n_observations, self.n_actions).to(CONST_DEVICE)\n","        # Вспомогательная модель, используется для стабилизации алгоритма\n","        # Обновление контролируется гиперпараметром TAU\n","        # Используется подход Double DQN\n","        self.target_net = DQN_Model(self.n_observations, self.n_actions).to(CONST_DEVICE)\n","        self.target_net.load_state_dict(self.policy_net.state_dict())\n","        # Оптимизатор\n","        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)\n","        #lambda1 = lambda epoch: 0.9**(epoch)\n","        #self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda1)\n","        #self.scheduler = lr_scheduler.ExponentialLR(self.optimizer, gamma=0.99)\n","        # Replay Memory\n","        self.memory = ReplayMemory(10000)\n","        # Количество шагов\n","        self.steps_done = 0\n","        # Длительность эпизодов\n","        self.episode_durations = []\n","\n","\n","    def select_action(self, state):\n","        '''\n","        Выбор действия\n","        '''\n","        sample = random.random()\n","        eps = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n","            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n","        self.steps_done += 1\n","        if sample > eps:\n","            with torch.no_grad():\n","                # Если вероятность больше eps\n","                # то выбирается действие, соответствующее максимальному Q-значению\n","                # t.max(1) возвращает максимальное значение колонки для каждой строки\n","                # [1] возвращает индекс максимального элемента\n","                return self.policy_net(state).max(1)[1].view(1, 1)\n","        else:\n","            # Если вероятность меньше eps\n","            # то выбирается случайное действие\n","            return torch.tensor([[self.env.action_space.sample()]], device=CONST_DEVICE, dtype=torch.long)\n","\n","\n","    def plot_durations(self, show_result=False):\n","        plt.figure(1)\n","        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n","        if show_result:\n","            plt.title('Результат')\n","        else:\n","            plt.clf()\n","            plt.title('Обучение...')\n","        plt.xlabel('Эпизод')\n","        plt.ylabel('Количество шагов в эпизоде')\n","        plt.plot(durations_t.numpy())\n","        plt.pause(0.001)  # пауза\n","\n","\n","    def optimize_model(self):\n","        '''\n","        Оптимизация модели\n","        '''\n","        if len(self.memory) < self.BATCH_SIZE:\n","            return\n","        transitions = self.memory.sample(self.BATCH_SIZE)\n","        # Транспонирование batch'а\n","        # (https://stackoverflow.com/a/19343/3343043)\n","        # Конвертация batch-массива из Transition\n","        # в Transition batch-массивов.\n","        batch = Transition(*zip(*transitions))\n","\n","        # Вычисление маски нефинальных состояний и конкатенация элементов batch'а\n","        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n","                                            batch.next_state)), device=CONST_DEVICE, dtype=torch.bool)\n","        non_final_next_states = torch.cat([s for s in batch.next_state\n","                                                    if s is not None])\n","        state_batch = torch.cat(batch.state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","\n","        # Вычисление Q(s_t, a)\n","        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n","\n","        # Вычисление V(s_{t+1}) для всех следующих состояний\n","        next_state_values = torch.zeros(self.BATCH_SIZE, device=CONST_DEVICE)\n","        with torch.no_grad():\n","            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n","        # Вычисление ожидаемых значений Q\n","        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n","\n","        # Вычисление Huber loss\n","        criterion = nn.SmoothL1Loss()\n","        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n","\n","        # Оптимизация модели\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        # gradient clipping\n","        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n","        self.optimizer.step()\n","        #self.scheduler.step()\n","\n","\n","    def play_agent(self):\n","        '''\n","        Проигрывание сессии для обученного агента\n","        '''\n","        env2 = gym.make(CONST_ENV_NAME, render_mode='human')\n","        state = env2.reset()[0]\n","        state = torch.tensor(state, dtype=torch.float32, device=CONST_DEVICE).unsqueeze(0)\n","        done = False\n","        res = []\n","        while not done:\n","\n","            action = self.select_action(state)\n","            action = action.item()\n","            observation, reward, terminated, truncated, _ = env2.step(action)\n","            env2.render()\n","\n","            res.append((action, reward))\n","\n","            if terminated:\n","                next_state = None\n","            else:\n","                next_state = torch.tensor(observation, dtype=torch.float32, device=CONST_DEVICE).unsqueeze(0)\n","\n","            state = next_state\n","            if terminated or truncated:\n","                done = True\n","\n","        print('Данные об эпизоде: ', res)\n","\n","\n","    def learn(self):\n","        '''\n","        Обучение агента\n","        '''\n","        if torch.cuda.is_available():\n","            num_episodes = 600\n","        else:\n","            num_episodes = 150\n","\n","        for i_episode in range(num_episodes):\n","            # Инициализация среды\n","            state, info = self.env.reset()\n","            state = torch.tensor(state, dtype=torch.float32, device=CONST_DEVICE).unsqueeze(0)\n","            for t in count():\n","                action = self.select_action(state)\n","                observation, reward, terminated, truncated, _ = self.env.step(action.item())\n","                reward = torch.tensor([reward], device=CONST_DEVICE)\n","\n","                done = terminated or truncated\n","                if terminated:\n","                    next_state = None\n","                else:\n","                    next_state = torch.tensor(observation, dtype=torch.float32, device=CONST_DEVICE).unsqueeze(0)\n","\n","                # Сохранение данных в Replay Memory\n","                self.memory.push(state, action, next_state, reward)\n","\n","                # Переход к следующему состоянию\n","                state = next_state\n","\n","                # Выполнение одного шага оптимизации модели\n","                self.optimize_model()\n","\n","                # Обновление весов target-сети\n","                # θ′ ← τ θ + (1 − τ )θ′\n","                target_net_state_dict = self.target_net.state_dict()\n","                policy_net_state_dict = self.policy_net.state_dict()\n","                for key in policy_net_state_dict:\n","                    target_net_state_dict[key] = policy_net_state_dict[key]*self.TAU + target_net_state_dict[key]*(1-self.TAU)\n","                self.target_net.load_state_dict(target_net_state_dict)\n","\n","                if done:\n","                    self.episode_durations.append(t + 1)\n","                    self.plot_durations()\n","                    break"],"metadata":{"id":"El03t9ck7Ejs","executionInfo":{"status":"ok","timestamp":1717853247715,"user_tz":-180,"elapsed":324,"user":{"displayName":"Roman Kuzmin","userId":"11647731679283067401"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["env = gym.make(CONST_ENV_NAME)\n","agent = DQN_Agent(env)\n","agent.learn()\n","agent.play_agent()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1uXCaNmNG3BChKDD92s9Nzd6EEPq4tnMb"},"id":"PLEkp_2q7M_U","executionInfo":{"status":"ok","timestamp":1717853301312,"user_tz":-180,"elapsed":51883,"user":{"displayName":"Roman Kuzmin","userId":"11647731679283067401"}},"outputId":"17b40e24-56d9-4a54-cc11-64700323aca9"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}